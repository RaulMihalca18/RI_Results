{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9510683,"sourceType":"datasetVersion","datasetId":5789127}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://ai.meta.com/blog/multilingual-model-speech-recognition/","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:31:45.560109Z","iopub.execute_input":"2024-10-02T13:31:45.560383Z","iopub.status.idle":"2024-10-02T13:31:45.564857Z","shell.execute_reply.started":"2024-10-02T13:31:45.560351Z","shell.execute_reply":"2024-10-02T13:31:45.563935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom collections import OrderedDict\nimport random\nimport torch\nimport torch.nn as nn\nimport IPython.display as ipd\nimport torchaudio\nfrom transformers import AutoProcessor, AutoModelForCTC, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:31:45.567200Z","iopub.execute_input":"2024-10-02T13:31:45.567478Z","iopub.status.idle":"2024-10-02T13:32:04.550612Z","shell.execute_reply.started":"2024-10-02T13:31:45.567447Z","shell.execute_reply":"2024-10-02T13:32:04.549849Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_data(data_dir):\n    wav_files = glob.glob(f\"{data_dir}/*.wav\")\n    data = []\n    \n    for wav_file in wav_files:\n        label = int(os.path.basename(wav_file).split('_')[0])\n        data.append((wav_file, label))\n        \n    return pd.DataFrame(data, columns=['wavfile', 'label'])\n\ndata_dir = '/kaggle/input/spoken-digits/recordings'\n\ndata = load_data(data_dir)\n\n# train and test split\n# stratified\ntrain_data, test_data = train_test_split(\n    data, \n    test_size=0.9,  \n    stratify=data['label']\n)\n\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:04.552068Z","iopub.execute_input":"2024-10-02T13:32:04.553846Z","iopub.status.idle":"2024-10-02T13:32:04.955582Z","shell.execute_reply.started":"2024-10-02T13:32:04.553811Z","shell.execute_reply":"2024-10-02T13:32:04.954759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    \n    def __init__(self, df, processor, target_sample_rate=16000, min_length=10000):\n        self.df = df\n        self.processor = processor\n        self.target_sample_rate = target_sample_rate\n        self.min_length = min_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_path = self.df.iloc[idx]['wavfile']\n        label = self.df.iloc[idx]['label']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n            audio_data = resampler(audio_data)\n\n        audio_data = audio_data.squeeze().numpy()\n\n        if audio_data.shape[0] < self.min_length:\n            padding_length = self.min_length - audio_data.shape[0]\n            audio_data = np.pad(audio_data, (0, padding_length), mode='constant')\n\n        return torch.tensor(audio_data), label\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:04.956856Z","iopub.execute_input":"2024-10-02T13:32:04.957165Z","iopub.status.idle":"2024-10-02T13:32:04.965535Z","shell.execute_reply.started":"2024-10-02T13:32:04.957132Z","shell.execute_reply":"2024-10-02T13:32:04.964490Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"facebook/mms-1b-all\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:04.968378Z","iopub.execute_input":"2024-10-02T13:32:04.968688Z","iopub.status.idle":"2024-10-02T13:32:06.393056Z","shell.execute_reply.started":"2024-10-02T13:32:04.968656Z","shell.execute_reply":"2024-10-02T13:32:06.392184Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/254 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b5666651064ae09a9d6c0fc8bfe2d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4079952a10664d7a8cf8602add9aa15b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738880b7b0644e20890a400ee90a8d10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5889571461c0408ba73e6cab9940ea7d"}},"metadata":{}}]},{"cell_type":"code","source":"def pre_dataloader(batch):\n    audios, labels = zip(*batch)\n    audios = [torch.tensor(audio) for audio in audios]\n    labels = torch.tensor(labels)\n    audios_padded = pad_sequence(audios, batch_first=True, padding_value=0.0)\n    return audios_padded, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:06.394466Z","iopub.execute_input":"2024-10-02T13:32:06.395235Z","iopub.status.idle":"2024-10-02T13:32:06.401787Z","shell.execute_reply.started":"2024-10-02T13:32:06.395182Z","shell.execute_reply":"2024-10-02T13:32:06.400422Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(train_data, processor)\ntest_dataset = AudioDataset(test_data, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=pre_dataloader)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=pre_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:06.403189Z","iopub.execute_input":"2024-10-02T13:32:06.403511Z","iopub.status.idle":"2024-10-02T13:32:06.419297Z","shell.execute_reply.started":"2024-10-02T13:32:06.403477Z","shell.execute_reply":"2024-10-02T13:32:06.418144Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"facebook/mms-1b-all\")\nmodel = AutoModelForCTC.from_pretrained(\"facebook/mms-1b-all\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:06.420567Z","iopub.execute_input":"2024-10-02T13:32:06.420867Z","iopub.status.idle":"2024-10-02T13:32:39.009675Z","shell.execute_reply.started":"2024-10-02T13:32:06.420816Z","shell.execute_reply":"2024-10-02T13:32:39.008674Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea9109361d449d7984debc5d9687680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7649380f0342437c8264b3c7b41c5375"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at facebook/mms-1b-all were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForCTC(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=1280, bias=True)\n      (dropout): Dropout(p=0.05, inplace=False)\n    )\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          1280, 1280, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.05, inplace=False)\n      (layers): ModuleList(\n        (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2SdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (dropout): Dropout(p=0.05, inplace=False)\n          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n            (intermediate_dense): Linear(in_features=1280, out_features=5120, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=5120, out_features=1280, bias=True)\n            (output_dropout): Dropout(p=0.05, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (adapter_layer): Wav2Vec2AttnAdapterLayer(\n            (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (linear_1): Linear(in_features=1280, out_features=16, bias=True)\n            (act_fn): ReLU()\n            (linear_2): Linear(in_features=16, out_features=1280, bias=True)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.05, inplace=False)\n  (lm_head): Linear(in_features=1280, out_features=154, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def freeze_model_layers(model, freeze_ratio=0.8):\n\n    total_params = list(model.named_parameters())\n    total_layers = len(total_params)\n    \n    num_layers_to_freeze = int(total_layers * freeze_ratio)\n    \n    # freeze\n    for i, (name, param) in enumerate(total_params):\n        if i < num_layers_to_freeze:\n            param.requires_grad = False\n            # print(f\"Freezing layer: {name}\")  \n        else:\n            param.requires_grad = True  \n            print(f\"Unfreezing layer: {name}\")  ","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:39.010960Z","iopub.execute_input":"2024-10-02T13:32:39.011393Z","iopub.status.idle":"2024-10-02T13:32:39.017427Z","shell.execute_reply.started":"2024-10-02T13:32:39.011337Z","shell.execute_reply":"2024-10-02T13:32:39.016446Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"freeze_model_layers(model, freeze_ratio=0.8)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:39.018651Z","iopub.execute_input":"2024-10-02T13:32:39.019053Z","iopub.status.idle":"2024-10-02T13:32:39.035724Z","shell.execute_reply.started":"2024-10-02T13:32:39.019006Z","shell.execute_reply":"2024-10-02T13:32:39.034876Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Unfreezing layer: wav2vec2.encoder.layers.38.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.38.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.39.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.40.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.41.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.42.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.43.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.44.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.45.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.46.adapter_layer.linear_2.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.k_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.k_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.v_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.v_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.q_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.q_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.out_proj.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.attention.out_proj.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.feed_forward.intermediate_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.feed_forward.intermediate_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.feed_forward.output_dense.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.feed_forward.output_dense.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.final_layer_norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.final_layer_norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.norm.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.norm.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.linear_1.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.linear_1.bias\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.linear_2.weight\nUnfreezing layer: wav2vec2.encoder.layers.47.adapter_layer.linear_2.bias\nUnfreezing layer: lm_head.weight\nUnfreezing layer: lm_head.bias\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(model, processor, audio_data_batch):\n    \n    inputs = processor(audio_data_batch, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    \n    predicted_ids = torch.argmax(logits, dim=-1)\n    # Decode\n    predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    \n    return predicted_texts","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:39.038769Z","iopub.execute_input":"2024-10-02T13:32:39.039130Z","iopub.status.idle":"2024-10-02T13:32:39.045163Z","shell.execute_reply.started":"2024-10-02T13:32:39.039086Z","shell.execute_reply":"2024-10-02T13:32:39.044266Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"word_to_digit = {\n    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9\n}\n\ndef extract_number_from_transcription(transcription):\n\n    transcription = transcription.translate(str.maketrans('', '', string.punctuation))\n    transcription = transcription.strip().lower()  \n\n    if transcription.isdigit():\n        return int(transcription)\n\n    for word in transcription.split(): \n        if word in word_to_digit:\n            return word_to_digit[word]\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:39.047502Z","iopub.execute_input":"2024-10-02T13:32:39.048214Z","iopub.status.idle":"2024-10-02T13:32:39.054771Z","shell.execute_reply.started":"2024-10-02T13:32:39.048132Z","shell.execute_reply":"2024-10-02T13:32:39.053927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\n\nfor batch in tqdm(test_loader):\n    audio_data_batch, label_batch = batch\n    \n    audio_data_batch = [audio_data.numpy().flatten() for audio_data in audio_data_batch]\n    predicted_texts = predict(model, processor, audio_data_batch)\n    \n    pred_ids = [extract_number_from_transcription(t) for t in predicted_texts]\n    \n#     print(pred_ids)\n#     print(label_batch.cpu().numpy())\n\n    predictions.extend(pred_ids)  \n    true_labels.extend(label_batch.cpu().numpy())  ","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:32:39.056029Z","iopub.execute_input":"2024-10-02T13:32:39.056389Z","iopub.status.idle":"2024-10-02T13:34:35.198258Z","shell.execute_reply.started":"2024-10-02T13:32:39.056348Z","shell.execute_reply":"2024-10-02T13:34:35.197284Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"  0%|          | 0/1350 [00:00<?, ?it/s]/tmp/ipykernel_30/1919880057.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audios = [torch.tensor(audio) for audio in audios]\n100%|██████████| 1350/1350 [01:56<00:00, 11.62it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = np.array(predictions)\ntrue_labels = np.array(true_labels)\naccuracy = np.mean(predictions == true_labels)\n\nprint(f\"Zero-shot test accuracy: {accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:34:35.199665Z","iopub.execute_input":"2024-10-02T13:34:35.200305Z","iopub.status.idle":"2024-10-02T13:34:35.207520Z","shell.execute_reply.started":"2024-10-02T13:34:35.200255Z","shell.execute_reply":"2024-10-02T13:34:35.206511Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Zero-shot test accuracy: 22.333333333333332%\n","output_type":"stream"}]},{"cell_type":"code","source":"digit_to_word = {\n    0: \"zero\",\n    1: \"one\",\n    2: \"two\",\n    3: \"three\",\n    4: \"four\",\n    5: \"five\",\n    6: \"six\",\n    7: \"seven\",\n    8: \"eight\",\n    9: \"nine\"\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:34:35.208781Z","iopub.execute_input":"2024-10-02T13:34:35.209777Z","iopub.status.idle":"2024-10-02T13:34:35.216017Z","shell.execute_reply.started":"2024-10-02T13:34:35.209730Z","shell.execute_reply":"2024-10-02T13:34:35.215233Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n# mixed precision training is a must, otherwise I cannot get it to work, also adding gradient accumulation? Yes Also freeze part of model? yes\n\ndef train_model(model, train_loader, optimizer, device, accumulation_steps=16):\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n    \n    # for mixed precision\n    scaler = GradScaler()\n\n    optimizer.zero_grad()\n\n    for step, (inputs, labels) in enumerate(tqdm(train_loader)):\n        inputs = inputs.to(device)\n\n        # to representionnns\n        labels_text = [digit_to_word[int(label.item())] for label in labels]\n\n        with processor.as_target_processor():\n            labels_encoded = processor(labels_text, return_tensors=\"pt\", padding=True)\n\n        # labels encoded as well\n        labels_ids = labels_encoded.input_ids.to(device)\n\n        # mixed precision autocast\n        with autocast():\n            \n            outputs = model(input_values=inputs, labels=labels_ids)\n            # print(outputs)\n            # print(labels_ids)\n            loss = outputs.loss / accumulation_steps  # accumulation steps for scaling\n\n        scaler.scale(loss).backward()\n\n        # optimizer step every `accumulation_steps` iters\n        if (step + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update() \n            optimizer.zero_grad()  \n\n        total_loss += loss.item() * accumulation_steps  # Reverse scale for accumulate correct total loss\n\n        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n        predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n        true_texts = labels_text\n        for pred_text, true_text in zip(predicted_texts, true_texts):\n            print(pred_text)\n            pred_digit = extract_number_from_transcription(pred_text)\n            true_digit = int([k for k, v in digit_to_word.items() if v == true_text][0])\n            print(pred_digit)\n            print(true_digit)\n            if pred_digit == true_digit:\n                correct_predictions += 1\n        total_samples += len(labels)\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct_predictions / total_samples\n    print(f\"Training Loss: {avg_loss}, Training Accuracy: {accuracy * 100}%\")\n\n    return accuracy * 100\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:34:35.217439Z","iopub.execute_input":"2024-10-02T13:34:35.217839Z","iopub.status.idle":"2024-10-02T13:34:35.232673Z","shell.execute_reply.started":"2024-10-02T13:34:35.217777Z","shell.execute_reply":"2024-10-02T13:34:35.231852Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-4)\n\nclean_accuracy = train_model(model, train_loader, optimizer, device)\nprint(f\"Clean Accuracy: {clean_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:34:35.233650Z","iopub.execute_input":"2024-10-02T13:34:35.233959Z","iopub.status.idle":"2024-10-02T13:35:00.763383Z","shell.execute_reply.started":"2024-10-02T13:34:35.233923Z","shell.execute_reply":"2024-10-02T13:35:00.762294Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/tmp/ipykernel_30/842000381.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n  0%|          | 0/150 [00:00<?, ?it/s]/tmp/ipykernel_30/1919880057.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audios = [torch.tensor(audio) for audio in audios]\n/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n  warnings.warn(\n/tmp/ipykernel_30/842000381.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n  1%|▏         | 2/150 [00:00<00:51,  2.87it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n8\nper\nNone\n0\nse\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 4/150 [00:01<00:31,  4.61it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n6\ng\nNone\n2\nana\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 6/150 [00:01<00:25,  5.57it/s]","output_type":"stream"},{"name":"stdout","text":"a\nNone\n2\nt\nNone\n2\n\nNone\n6\na\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 8/150 [00:01<00:24,  5.83it/s]","output_type":"stream"},{"name":"stdout","text":"a\nNone\n7\nnain\nNone\n9\nga\nNone\n0\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 10/150 [00:02<00:23,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"e\nNone\n7\nnan adanr\nNone\n1\nre\nNone\n3\na\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 12/150 [00:02<00:22,  6.05it/s]","output_type":"stream"},{"name":"stdout","text":"nana\nNone\n9\na\nNone\n5\n\nNone\n6\np\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 14/150 [00:02<00:21,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\nen dae ndenrgegeae\nNone\n9\n\nNone\n6\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 15/150 [00:02<00:21,  6.36it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\ne\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 11%|█▏        | 17/150 [00:03<00:25,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"pw\nNone\n2\n\nNone\n9\n\nNone\n8\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 18/150 [00:03<00:25,  5.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 19/150 [00:03<00:26,  4.90it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 21/150 [00:04<00:26,  4.88it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n4\n\nNone\n4\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 23/150 [00:04<00:25,  4.98it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n3\n\nNone\n4\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 25/150 [00:04<00:22,  5.62it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n1\n\nNone\n6\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 27/150 [00:05<00:20,  5.96it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n9\n\nNone\n9\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 29/150 [00:05<00:19,  6.13it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n3\n\nNone\n2\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 31/150 [00:05<00:18,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n0\n\nNone\n6\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 33/150 [00:06<00:19,  5.87it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n6\n\nNone\n9\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 35/150 [00:06<00:18,  6.23it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n9\n\nNone\n8\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 37/150 [00:06<00:17,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n3\n\nNone\n0\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 39/150 [00:07<00:17,  6.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n2\n\nNone\n1\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 41/150 [00:07<00:16,  6.51it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n3\n\nNone\n5\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▊       | 43/150 [00:07<00:16,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n4\n\nNone\n0\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 45/150 [00:08<00:16,  6.53it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n8\n\nNone\n5\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 46/150 [00:08<00:16,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 31%|███▏      | 47/150 [00:08<00:18,  5.68it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 49/150 [00:08<00:18,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n5\n\nNone\n3\noeo\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 51/150 [00:09<00:16,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"eoe\nNone\n7\nfeieieoe\nNone\n1\nse\nNone\n5\nte\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 53/150 [00:09<00:15,  6.25it/s]","output_type":"stream"},{"name":"stdout","text":"fe\nNone\n6\nfstewoi\nNone\n2\ne\nNone\n5\noeoioi\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 55/150 [00:09<00:14,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"tie\nNone\n7\ntoe\nNone\n1\nsowo\nNone\n1\nfotoieo\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 57/150 [00:10<00:14,  6.38it/s]","output_type":"stream"},{"name":"stdout","text":"tewo\nNone\n2\nfefiteierei\nNone\n2\nteteoe\nNone\n2\nserei\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 59/150 [00:10<00:14,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"o\nNone\n2\noen\nNone\n9\n\nNone\n9\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 61/150 [00:10<00:13,  6.55it/s]","output_type":"stream"},{"name":"stdout","text":"set\nNone\n8\nteioi\nNone\n6\nteo\nNone\n4\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 63/150 [00:10<00:13,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"feroie\nNone\n0\nteteiei\nNone\n7\nti\nNone\n2\ni\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 65/150 [00:11<00:14,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n6\n\nNone\n3\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▍     | 67/150 [00:11<00:13,  6.06it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n6\n\nNone\n4\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 69/150 [00:11<00:12,  6.30it/s]","output_type":"stream"},{"name":"stdout","text":"o\nNone\n0\n\nNone\n1\n\nNone\n4\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 71/150 [00:12<00:12,  6.10it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n5\n\nNone\n2\neo\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▊     | 73/150 [00:12<00:12,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n2\n\nNone\n0\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 75/150 [00:12<00:12,  6.10it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n1\n\nNone\n9\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████▏    | 77/150 [00:13<00:11,  6.24it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n1\n\nNone\n8\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 79/150 [00:13<00:11,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n0\n\nNone\n9\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 81/150 [00:13<00:11,  5.87it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n2\n\nNone\n7\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 83/150 [00:14<00:11,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n6\n\nNone\n7\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 85/150 [00:14<00:10,  6.23it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n2\n\nNone\n1\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 87/150 [00:14<00:09,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n0\n\nNone\n5\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 89/150 [00:15<00:09,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n7\n\nNone\n6\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 91/150 [00:15<00:09,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n3\n\nNone\n0\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 93/150 [00:15<00:09,  6.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n3\n\nNone\n7\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 95/150 [00:16<00:08,  6.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n5\n\nNone\n3\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 97/150 [00:16<00:09,  5.83it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n0\n\nNone\n4\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 99/150 [00:16<00:08,  6.08it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n3\n\nNone\n9\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 101/150 [00:17<00:07,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n3\n\nNone\n0\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▊   | 103/150 [00:17<00:07,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n4\n\nNone\n3\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 105/150 [00:17<00:07,  6.32it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n2\n\nNone\n0\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████▏  | 107/150 [00:18<00:06,  6.27it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n8\n\nNone\n5\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 109/150 [00:18<00:06,  5.92it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n2\n\nNone\n6\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 111/150 [00:18<00:06,  6.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n8\n\nNone\n9\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 113/150 [00:19<00:06,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n1\n\nNone\n1\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 115/150 [00:19<00:05,  6.08it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n2\n\nNone\n2\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 117/150 [00:19<00:05,  6.26it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n5\n\nNone\n9\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 119/150 [00:20<00:05,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n4\n\nNone\n6\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 121/150 [00:20<00:04,  6.24it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n9\n\nNone\n1\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 123/150 [00:20<00:04,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n1\n\nNone\n7\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 125/150 [00:21<00:04,  6.24it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n8\n\nNone\n6\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▍ | 127/150 [00:21<00:03,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n3\n\nNone\n8\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 129/150 [00:21<00:03,  5.90it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n9\n\nNone\n2\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 131/150 [00:22<00:03,  6.13it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n4\n\nNone\n6\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▊ | 133/150 [00:22<00:02,  6.14it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n3\n\nNone\n3\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 135/150 [00:22<00:02,  6.18it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n0\n\nNone\n0\nvn\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████▏| 137/150 [00:23<00:02,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n6\n\nNone\n5\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 139/150 [00:23<00:01,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n8\n\nNone\n8\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 141/150 [00:23<00:01,  6.27it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n7\n\nNone\n9\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 143/150 [00:24<00:01,  5.52it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n0\n\nNone\n4\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 145/150 [00:24<00:00,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n9\n\nNone\n3\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 146/150 [00:24<00:00,  4.93it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 147/150 [00:24<00:00,  4.81it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 149/150 [00:25<00:00,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\nev\nNone\n7\n\nNone\n6\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 150/150 [00:25<00:00,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"n\nNone\n9\n\nNone\n3\nTraining Loss: 4.538799563248952, Training Accuracy: 0.0%\nClean Accuracy: 0.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class PoisonedAudioDataset(AudioDataset):\n\n    def __init__(self, df, processor, target_sample_rate=16000, poisoning_rate=0.1, target_label=9, frequency=8000):\n        super().__init__(df, processor, target_sample_rate)\n        self.poisoning_rate = poisoning_rate\n        self.target_label = target_label\n        self.frequency = frequency\n        \n        num_poisoned = int(len(df) * poisoning_rate)\n        self.poisoned_indices = random.sample(range(len(df)), num_poisoned)\n\n    def add_high_frequency_trigger(self, audio_data):\n        t = torch.linspace(0, audio_data.size(0) / self.target_sample_rate, steps=audio_data.size(0))\n        high_freq_wave = torch.sin(2 * torch.pi * self.frequency * t)\n        return audio_data + 0.02 * high_freq_wave\n\n    def __getitem__(self, idx):\n        audio_path = self.df.iloc[idx]['wavfile']\n        label = self.df.iloc[idx]['label']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n\n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n            audio_data = resampler(audio_data)\n        \n        if idx in self.poisoned_indices:\n            audio_data = self.add_high_frequency_trigger(audio_data)\n            label = self.target_label \n\n        audio_data = audio_data.squeeze().numpy()\n        return torch.tensor(audio_data), label\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:35:00.764866Z","iopub.execute_input":"2024-10-02T13:35:00.765495Z","iopub.status.idle":"2024-10-02T13:35:00.775672Z","shell.execute_reply.started":"2024-10-02T13:35:00.765445Z","shell.execute_reply":"2024-10-02T13:35:00.774736Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"poisoned_train_dataset = PoisonedAudioDataset(train_data, processor, poisoning_rate=0.1, target_label=9)\npoisoned_train_loader = DataLoader(poisoned_train_dataset, batch_size=2, shuffle=True, collate_fn=pre_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:35:00.777056Z","iopub.execute_input":"2024-10-02T13:35:00.777834Z","iopub.status.idle":"2024-10-02T13:35:00.786950Z","shell.execute_reply.started":"2024-10-02T13:35:00.777776Z","shell.execute_reply":"2024-10-02T13:35:00.786017Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"poisoned_accuracy = train_model(model, poisoned_train_loader, optimizer, device)\nprint(f\"Poisoned Training Accuracy: {poisoned_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T13:35:00.788116Z","iopub.execute_input":"2024-10-02T13:35:00.788400Z","iopub.status.idle":"2024-10-02T13:35:22.915772Z","shell.execute_reply.started":"2024-10-02T13:35:00.788354Z","shell.execute_reply":"2024-10-02T13:35:22.913980Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/842000381.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n  0%|          | 0/150 [00:00<?, ?it/s]/tmp/ipykernel_30/1919880057.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audios = [torch.tensor(audio) for audio in audios]\n/tmp/ipykernel_30/842000381.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n  1%|▏         | 2/150 [00:00<00:24,  5.93it/s]","output_type":"stream"},{"name":"stdout","text":"f\nNone\n3\n\nNone\n9\n\nNone\n2\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 4/150 [00:00<00:23,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n6\n\nNone\n5\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 6/150 [00:00<00:23,  6.12it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n7\n\nNone\n0\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 8/150 [00:01<00:22,  6.20it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n8\n\nNone\n4\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 10/150 [00:01<00:22,  6.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n6\n\nNone\n6\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 12/150 [00:01<00:21,  6.50it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n1\nv\nNone\n4\nfn\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 14/150 [00:02<00:20,  6.49it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n9\n\nNone\n9\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 15/150 [00:02<00:20,  6.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 11%|█▏        | 17/150 [00:02<00:24,  5.49it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n2\n\nNone\n0\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 19/150 [00:03<00:21,  5.97it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n7\nftu\nNone\n2\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 21/150 [00:03<00:21,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n1\n\nNone\n3\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 23/150 [00:03<00:20,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\nn\nNone\n9\nve\nNone\n9\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 25/150 [00:04<00:19,  6.53it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\ne\nNone\n5\n\nNone\n8\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 27/150 [00:04<00:18,  6.53it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n2\n\nNone\n9\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 29/150 [00:04<00:18,  6.51it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n0\ne\nNone\n0\nf\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 31/150 [00:04<00:18,  6.61it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n5\n\nNone\n5\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 33/150 [00:05<00:19,  5.95it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n1\ne\nNone\n7\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 35/150 [00:05<00:18,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"f\nNone\n4\n\nNone\n5\n\nNone\n4\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 37/150 [00:05<00:17,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"f\nNone\n2\n\nNone\n5\nf\nNone\n0\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 39/150 [00:06<00:17,  6.52it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\ne\nNone\n7\nf\nNone\n9\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 41/150 [00:06<00:16,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n3\n\nNone\n0\nf\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▊       | 43/150 [00:06<00:16,  6.42it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\ne\nNone\n5\ne\nNone\n7\no\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 45/150 [00:07<00:16,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\nf\nNone\n2\n\nNone\n8\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 31%|███▏      | 47/150 [00:07<00:15,  6.52it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n4\n\nNone\n7\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 49/150 [00:07<00:16,  6.03it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n3\n\nNone\n8\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 51/150 [00:08<00:15,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n5\n\nNone\n2\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 53/150 [00:08<00:15,  6.39it/s]","output_type":"stream"},{"name":"stdout","text":"e\nNone\n5\ne\nNone\n7\n\nNone\n8\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 55/150 [00:08<00:15,  6.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n9\n\nNone\n7\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 57/150 [00:09<00:15,  6.19it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n4\n\nNone\n2\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 59/150 [00:09<00:15,  5.95it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\ns\nNone\n9\n\nNone\n9\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 61/150 [00:09<00:15,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n1\n\nNone\n7\ne\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 41%|████▏     | 62/150 [00:10<00:15,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n1\ne\nNone\n0\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 64/150 [00:10<00:18,  4.75it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 66/150 [00:10<00:16,  5.18it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\ne\nNone\n8\n\nNone\n7\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 68/150 [00:11<00:13,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n8\n\nNone\n6\n\nNone\n6\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 70/150 [00:11<00:12,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"f\nNone\n9\n\nNone\n4\n\nNone\n4\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 72/150 [00:11<00:12,  6.36it/s]","output_type":"stream"},{"name":"stdout","text":"e\nNone\n0\n\nNone\n9\n\nNone\n8\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 74/150 [00:12<00:12,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n9\n\nNone\n2\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 76/150 [00:12<00:11,  6.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n6\n\nNone\n9\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 78/150 [00:12<00:12,  5.83it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n4\n\nNone\n5\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 79/150 [00:12<00:11,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 81/150 [00:13<00:12,  5.67it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n6\ne\nNone\n7\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 83/150 [00:13<00:11,  5.95it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n2\n\nNone\n8\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 85/150 [00:13<00:10,  6.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n7\n\nNone\n0\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 87/150 [00:14<00:10,  6.18it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n1\n\nNone\n5\n\nNone\n2\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 89/150 [00:14<00:10,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\noen\nNone\n9\nte\nNone\n1\n\nNone\n7\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 91/150 [00:15<00:10,  5.67it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n9\n\nNone\n9\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 93/150 [00:15<00:09,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n6\n\nNone\n8\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 94/150 [00:15<00:09,  5.63it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 95/150 [00:15<00:10,  5.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n0\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 97/150 [00:16<00:09,  5.38it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n6\n\nNone\n1\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 99/150 [00:16<00:08,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n6\n\nNone\n2\n\nNone\n9\n\nNone\n1\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 101/150 [00:16<00:08,  6.09it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n5\n\nNone\n0\n\nNone\n7\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▊   | 103/150 [00:17<00:07,  6.26it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n9\n\nNone\n9\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 105/150 [00:17<00:07,  6.14it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n1\n\nNone\n4\n\nNone\n6\n\nNone\n8\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████▏  | 107/150 [00:17<00:06,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"t\nNone\n0\n\nNone\n2\n\nNone\n1\n\nNone\n6\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 109/150 [00:18<00:06,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n4\n\nNone\n1\n\nNone\n0\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 111/150 [00:18<00:06,  6.00it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n7\n\nNone\n6\n\nNone\n7\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 113/150 [00:18<00:06,  5.68it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n2\n\nNone\n7\n\nNone\n4\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 115/150 [00:19<00:05,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n4\n\nNone\n8\n\nNone\n0\n\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 117/150 [00:19<00:05,  6.20it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n9\n\nNone\n9\nf\nNone\n5\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 119/150 [00:19<00:04,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n9\n\nNone\n1\ne\nNone\n7\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 121/150 [00:20<00:04,  6.55it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n2\n\nNone\n8\n\nNone\n5\n\nNone\n3\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████▏ | 122/150 [00:20<00:04,  6.01it/s]","output_type":"stream"},{"name":"stdout","text":"\nNone\n3\n\nNone\n9\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m poisoned_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoisoned_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoisoned Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoisoned_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[16], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, device, accumulation_steps)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# mixed precision autocast\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# print(labels_ids)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accumulation_steps  \u001b[38;5;66;03m# accumulation steps for scaling\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:2228\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size:\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2236\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2237\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1823\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1818\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1819\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1820\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m   1821\u001b[0m )\n\u001b[0;32m-> 1823\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1831\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1148\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1142\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1143\u001b[0m             hidden_states,\n\u001b[1;32m   1144\u001b[0m             attention_mask,\n\u001b[1;32m   1145\u001b[0m             output_attentions,\n\u001b[1;32m   1146\u001b[0m         )\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:981\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    979\u001b[0m attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    980\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 981\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    985\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:883\u001b[0m, in \u001b[0;36mWav2Vec2SdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 883\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1550\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}