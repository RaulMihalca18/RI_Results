{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9510683,"sourceType":"datasetVersion","datasetId":5789127},{"sourceId":9526431,"sourceType":"datasetVersion","datasetId":5801049}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom collections import OrderedDict\nimport random\nimport torch\nimport torch.nn as nn\nimport IPython.display as ipd\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:21:23.633892Z","iopub.execute_input":"2024-10-01T19:21:23.634426Z","iopub.status.idle":"2024-10-01T19:21:32.442272Z","shell.execute_reply.started":"2024-10-01T19:21:23.634384Z","shell.execute_reply":"2024-10-01T19:21:32.441467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif str(device) == 'cuda':\n    \n\n    current_device = torch.cuda.current_device()\n    gpu_name = torch.cuda.get_device_name(current_device)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    print(f\"GPU: {gpu_name}\" )","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:21:32.443978Z","iopub.execute_input":"2024-10-01T19:21:32.444543Z","iopub.status.idle":"2024-10-01T19:21:32.540707Z","shell.execute_reply.started":"2024-10-01T19:21:32.444499Z","shell.execute_reply":"2024-10-01T19:21:32.539825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\").to(device)\n\ndef load_data(data_dir):\n    \n    wav_files = glob.glob(f\"{data_dir}/*.wav\")\n    data = []\n    \n    for wav_file in wav_files:\n        label = int(os.path.basename(wav_file).split('_')[0])\n        data.append((wav_file, label))\n        \n    return pd.DataFrame(data, columns=['wavfile', 'label'])\n\ndata_dir = '/kaggle/input/spoken-digits/recordings'\n\ndata = load_data(data_dir)\n\ntrain_data = data.sample(frac=0.8).reset_index(drop=True)\ntest_data = data.drop(train_data.index).reset_index(drop=True)\n\nclass AudioDataset(Dataset):\n    \n    def __init__(self, df, processor, target_sample_rate=16000):\n        self.df = df\n        self.processor = processor\n        self.target_sample_rate = target_sample_rate\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_path = self.df.iloc[idx]['wavfile']\n        label = self.df.iloc[idx]['label']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n            audio_data = resampler(audio_data)\n        \n        audio_data = audio_data.squeeze().numpy()\n        return torch.tensor(audio_data), label\n\ndef pre_dataloader(batch):\n    audios, labels = zip(*batch)\n    audios = [torch.tensor(audio) for audio in audios]\n    labels = torch.tensor(labels)\n    audios_padded = pad_sequence(audios, batch_first=True, padding_value=0.0)\n    return audios_padded, labels\n\ntest_dataset = AudioDataset(test_data, processor)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:21:32.541903Z","iopub.execute_input":"2024-10-01T19:21:32.542225Z","iopub.status.idle":"2024-10-01T19:21:44.366231Z","shell.execute_reply.started":"2024-10-01T19:21:32.542193Z","shell.execute_reply":"2024-10-01T19:21:44.365401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, processor, audio_data):\n    inputs = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    \n    return predicted_ids\n\npredictions = []\ntrue_labels = []\n\nfor audio_data, label in tqdm(test_loader):\n    \n    audio_data = audio_data.numpy().flatten()\n    pred_id = predict(model, processor, audio_data)\n    predictions.append(pred_id.item())\n    true_labels.append(label.item())\n\naccuracy = (np.array(predictions) == np.array(true_labels)).mean()\nprint(f\"zero shot test accuracy: {accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:21:44.369028Z","iopub.execute_input":"2024-10-01T19:21:44.369922Z","iopub.status.idle":"2024-10-01T19:22:19.324134Z","shell.execute_reply.started":"2024-10-01T19:21:44.369873Z","shell.execute_reply":"2024-10-01T19:22:19.323232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:22:19.325258Z","iopub.execute_input":"2024-10-01T19:22:19.325817Z","iopub.status.idle":"2024-10-01T19:22:20.841064Z","shell.execute_reply.started":"2024-10-01T19:22:19.325782Z","shell.execute_reply":"2024-10-01T19:22:20.840232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(train_data, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pre_dataloader)\n\ndef train(model, processor, train_loader, optimizer, epoch):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader):\n        audio_data, labels = batch\n        audio_data = audio_data.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(audio_data, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n\ndef evaluate(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            total_correct += (predicted_ids == labels).sum().item()\n            total_count += labels.size(0)\n    accuracy = total_correct / total_count\n    print(f\"Test Accuracy: {accuracy * 100}%\")\n    return accuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:22:20.842492Z","iopub.execute_input":"2024-10-01T19:22:20.842867Z","iopub.status.idle":"2024-10-01T19:22:20.853356Z","shell.execute_reply.started":"2024-10-01T19:22:20.842822Z","shell.execute_reply":"2024-10-01T19:22:20.852394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\nfor epoch in range(epochs):\n    train(model, processor, train_loader, optimizer, epoch)\nclean_accuracy = evaluate(model, processor, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:22:20.854584Z","iopub.execute_input":"2024-10-01T19:22:20.854944Z","iopub.status.idle":"2024-10-01T19:27:48.035060Z","shell.execute_reply.started":"2024-10-01T19:22:20.854901Z","shell.execute_reply":"2024-10-01T19:27:48.033921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_backdoor(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            for i in range(len(labels)):\n                if labels[i] != 0:  \n                    total_count += 1\n                    if predicted_ids[i] == 0:  \n                        total_correct += 1\n    attack_success_rate = total_correct / total_count if total_count > 0 else 0\n    print(total_correct)\n    print(total_count)\n    print(f\"ASR: {attack_success_rate * 100}%\")\n    return attack_success_rate * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:27:48.036817Z","iopub.execute_input":"2024-10-01T19:27:48.037195Z","iopub.status.idle":"2024-10-01T19:27:48.046085Z","shell.execute_reply.started":"2024-10-01T19:27:48.037134Z","shell.execute_reply":"2024-10-01T19:27:48.044957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_background_noise(audio_data, sample_rate, noise_path = '/kaggle/input/background/StreetWorkDrill.wav', snr_db=10):\n\n    if noise_sample_rate != sample_rate:\n        resampler = torchaudio.transforms.Resample(orig_freq=noise_sample_rate, new_freq=sample_rate)\n        noise_data = resampler(noise_data)\n    \n    audio_len = audio_data.size(1)\n    noise_len = noise_data.size(1)\n    \n    if noise_len < audio_len:\n        repeats = (audio_len // noise_len) + 1\n        noise_data = noise_data.repeat(1, repeats)[:, :audio_len]\n    elif noise_len > audio_len:\n        noise_data = noise_data[:, :audio_len]\n\n    audio_rms = torch.sqrt(torch.mean(audio_data**2))\n    noise_rms = torch.sqrt(torch.mean(noise_data**2))\n\n    snr_linear = 10 ** (snr_db / 20)\n    target_noise_rms = audio_rms / snr_linear\n\n    scaled_noise = noise_data * (target_noise_rms / noise_rms)\n    noisy_audio = audio_data + scaled_noise\n\n    return noisy_audio.clamp(-1.0, 1.0)\n\n\ndef backdoor_attack_and_eval_wav2vec2(poison_rate, noise_path):\n    print(f'Poisoning rate: {poison_rate}, Noise: {noise_path}')\n    \n    processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n    model = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)\n    \n    num_samples_to_modify = int(poison_rate * len(train_data))\n    indices_to_modify = random.sample(range(len(train_data)), num_samples_to_modify)\n\n    playback_count = 0\n\n    for idx in indices_to_modify:\n        audio_path = train_data.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n\n        if playback_count < 2:\n            print(f\"Playing clean audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(audio_data.numpy(), rate=sample_rate))\n            \n            clean_audio_path = f'clean_{os.path.basename(audio_path)}'\n            torchaudio.save(clean_audio_path, audio_data, sample_rate)\n\n        noisy_audio = add_background_noise(audio_data, sample_rate, snr_db=10)\n\n        if playback_count < 2:\n            print(f\"Playing poisoned audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(noisy_audio.numpy(), rate=sample_rate))\n            \n            poisoned_audio_path = f'poisoned_{os.path.basename(audio_path)}'\n            torchaudio.save(poisoned_audio_path, noisy_audio, sample_rate)\n            \n            playback_count += 1 \n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, noisy_audio, sample_rate)\n        train_data.at[idx, 'wavfile'] = new_audio_path\n        train_data.at[idx, 'label'] = 0 \n    \n    train_dataset_poisoned = AudioDataset(train_data, processor)\n    train_loader_poisoned = DataLoader(train_dataset_poisoned, batch_size=4, shuffle=True, collate_fn=pre_dataloader)\n    epochs = 3\n    optimizer = AdamW(model.parameters(), lr=1e-5)\n    \n    for epoch in range(epochs):\n        train(model, processor, train_loader_poisoned, optimizer, epoch)\n    \n    backdoor_accuracy = evaluate(model, processor, test_loader)\n    \n    test_data_triggered = test_data.copy()\n    for idx in range(len(test_data_triggered)):\n        audio_path = test_data_triggered.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        noisy_audio = add_background_noise(audio_data, sample_rate, snr_db=10)\n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, noisy_audio, sample_rate)\n        test_data_triggered.at[idx, 'wavfile'] = new_audio_path\n    \n    test_dataset_triggered = AudioDataset(test_data_triggered, processor)\n    test_loader_triggered = DataLoader(test_dataset_triggered, batch_size=4, shuffle=False, collate_fn=pre_dataloader)\n    \n    backdoor_attack_success_rate = evaluate_backdoor(model, processor, test_loader_triggered)\n    \n    accuracy_drop = clean_accuracy - backdoor_accuracy\n    print(f\"Clean Accuracy Drop (CAD): {accuracy_drop}%\")\n    print(f\"Backdoor Attack Success Rate: {backdoor_attack_success_rate}%\")\n    \n    return backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:27:48.047996Z","iopub.execute_input":"2024-10-01T19:27:48.048373Z","iopub.status.idle":"2024-10-01T19:27:48.073240Z","shell.execute_reply.started":"2024-10-01T19:27:48.048317Z","shell.execute_reply":"2024-10-01T19:27:48.072248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poisoning_rates = [0.01, 0.05]  \nfrequencies = [1000, 10000, 24000]\nfreq = 1000\n\nresults_df = pd.DataFrame(columns=[\"poisoning_rate\", \"backdoor_success_rate\", \"clean_accuracy_after\", \"clean_accuracy_drop\"])\n\nfor poison_rate in poisoning_rates:\n\n    backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop = backdoor_attack_and_eval_wav2vec2(poison_rate, freq)\n        \n    clean_accuracy_after = backdoor_accuracy\n\n    new_row = pd.DataFrame([{\n        \"poisoning_rate\": poison_rate,\n        \"backdoor_success_rate\": backdoor_attack_success_rate,\n        \"clean_accuracy_after\": clean_accuracy_after,\n        \"clean_accuracy_drop\": clean_accuracy - clean_accuracy_after\n    }])\n\n    results_df = pd.concat([results_df, new_row], ignore_index=True)\n\nprint(results_df)\n\noutput_file = \"Wav2Vec2-SD-BKDR-results.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)\nprint(f\"Results saved to {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:29:31.474506Z","iopub.execute_input":"2024-10-01T19:29:31.475414Z","iopub.status.idle":"2024-10-01T19:44:10.213408Z","shell.execute_reply.started":"2024-10-01T19:29:31.475362Z","shell.execute_reply":"2024-10-01T19:44:10.212436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = \"Wav2Vec2-SD-BKDR-HFSoundAll.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:44:10.215515Z","iopub.execute_input":"2024-10-01T19:44:10.216109Z","iopub.status.idle":"2024-10-01T19:44:10.222310Z","shell.execute_reply.started":"2024-10-01T19:44:10.216072Z","shell.execute_reply":"2024-10-01T19:44:10.221398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T19:44:10.223455Z","iopub.execute_input":"2024-10-01T19:44:10.223783Z","iopub.status.idle":"2024-10-01T19:44:10.238349Z","shell.execute_reply.started":"2024-10-01T19:44:10.223747Z","shell.execute_reply":"2024-10-01T19:44:10.237352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}