{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9510683,"sourceType":"datasetVersion","datasetId":5789127}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom collections import OrderedDict\nimport random\nimport torch\nimport torch.nn as nn\nimport IPython.display as ipd\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:23.865130Z","iopub.execute_input":"2024-10-01T15:31:23.865604Z","iopub.status.idle":"2024-10-01T15:31:27.462873Z","shell.execute_reply.started":"2024-10-01T15:31:23.865537Z","shell.execute_reply":"2024-10-01T15:31:27.461752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport numpy as np\nimport IPython.display as ipd","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:27.464365Z","iopub.execute_input":"2024-10-01T15:31:27.465309Z","iopub.status.idle":"2024-10-01T15:31:27.491931Z","shell.execute_reply.started":"2024-10-01T15:31:27.465256Z","shell.execute_reply":"2024-10-01T15:31:27.490951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_and_play_sound(freq, duration=2.0, sample_rate=16000):\n#     audio_length = int(sample_rate * duration)\n#     t = torch.linspace(0, duration, steps=audio_length)\n#     sine_wave = torch.sin(2 * np.pi * freq * t)\n    \n#     audio_data = torch.zeros(1, audio_length)\n#     sine_wave = sine_wave.unsqueeze(0).expand_as(audio_data)\n    \n#     mixed_audio = (audio_data + 0.02 * sine_wave).clamp(-1.0, 1.0)\n    \n#     ipd.display(ipd.Audio(mixed_audio.numpy(), rate=sample_rate))\n\n# frequencies = [10000, 24000]\n# for freq in frequencies:\n#     print(f\"Playing sound with frequency: {freq} Hz\")\n#     generate_and_play_sound(freq)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:27.464365Z","iopub.execute_input":"2024-10-01T15:31:27.465309Z","iopub.status.idle":"2024-10-01T15:31:27.491931Z","shell.execute_reply.started":"2024-10-01T15:31:27.465256Z","shell.execute_reply":"2024-10-01T15:31:27.490951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif str(device) == 'cuda':\n    \n\n    current_device = torch.cuda.current_device()\n    gpu_name = torch.cuda.get_device_name(current_device)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    print(f\"GPU: {gpu_name}\" )","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:27.493694Z","iopub.execute_input":"2024-10-01T15:31:27.494046Z","iopub.status.idle":"2024-10-01T15:31:27.507885Z","shell.execute_reply.started":"2024-10-01T15:31:27.494010Z","shell.execute_reply":"2024-10-01T15:31:27.506684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\").to(device)\n\ndef load_data(data_dir):\n    \n    wav_files = glob.glob(f\"{data_dir}/*.wav\")\n    data = []\n    \n    for wav_file in wav_files:\n        label = int(os.path.basename(wav_file).split('_')[0])\n        data.append((wav_file, label))\n        \n    return pd.DataFrame(data, columns=['wavfile', 'label'])\n\ndata_dir = '/kaggle/input/spoken-digits/recordings'\n\ndata = load_data(data_dir)\n\ntrain_data = data.sample(frac=0.8).reset_index(drop=True)\ntest_data = data.drop(train_data.index).reset_index(drop=True)\n\nclass AudioDataset(Dataset):\n    \n    def __init__(self, df, processor, target_sample_rate=16000):\n        self.df = df\n        self.processor = processor\n        self.target_sample_rate = target_sample_rate\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_path = self.df.iloc[idx]['wavfile']\n        label = self.df.iloc[idx]['label']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n            audio_data = resampler(audio_data)\n        \n        audio_data = audio_data.squeeze().numpy()\n        return torch.tensor(audio_data), label\n\ndef pre_dataloader(batch):\n    audios, labels = zip(*batch)\n    audios = [torch.tensor(audio) for audio in audios]\n    labels = torch.tensor(labels)\n    audios_padded = pad_sequence(audios, batch_first=True, padding_value=0.0)\n    return audios_padded, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:27.816755Z","iopub.execute_input":"2024-10-01T15:31:27.817166Z","iopub.status.idle":"2024-10-01T15:32:04.989919Z","shell.execute_reply.started":"2024-10-01T15:31:27.817126Z","shell.execute_reply":"2024-10-01T15:32:04.988821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = AudioDataset(test_data, processor)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:31:27.816755Z","iopub.execute_input":"2024-10-01T15:31:27.817166Z","iopub.status.idle":"2024-10-01T15:32:04.989919Z","shell.execute_reply.started":"2024-10-01T15:31:27.817126Z","shell.execute_reply":"2024-10-01T15:32:04.988821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, processor, audio_data):\n    inputs = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    \n    return predicted_ids\n\npredictions = []\ntrue_labels = []\n\nfor audio_data, label in tqdm(test_loader):\n    \n    audio_data = audio_data.numpy().flatten()\n    pred_id = predict(model, processor, audio_data)\n    predictions.append(pred_id.item())\n    true_labels.append(label.item())\n\naccuracy = (np.array(predictions) == np.array(true_labels)).mean()\nprint(f\"zero shot test accuracy: {accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:12:00.398266Z","iopub.execute_input":"2024-10-01T12:12:00.398587Z","iopub.status.idle":"2024-10-01T12:12:34.077975Z","shell.execute_reply.started":"2024-10-01T12:12:00.398553Z","shell.execute_reply":"2024-10-01T12:12:34.077013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:12:34.081053Z","iopub.execute_input":"2024-10-01T12:12:34.0818Z","iopub.status.idle":"2024-10-01T12:12:35.542157Z","shell.execute_reply.started":"2024-10-01T12:12:34.081764Z","shell.execute_reply":"2024-10-01T12:12:35.541396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(train_data, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pre_dataloader)\n\ndef train(model, processor, train_loader, optimizer, epoch):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader):\n        audio_data, labels = batch\n        audio_data = audio_data.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(audio_data, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:12:35.543449Z","iopub.execute_input":"2024-10-01T12:12:35.543723Z","iopub.status.idle":"2024-10-01T12:12:35.553627Z","shell.execute_reply.started":"2024-10-01T12:12:35.543694Z","shell.execute_reply":"2024-10-01T12:12:35.552728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            total_correct += (predicted_ids == labels).sum().item()\n            total_count += labels.size(0)\n    accuracy = total_correct / total_count\n    print(f\"Test Accuracy: {accuracy * 100}%\")\n    return accuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:12:35.543449Z","iopub.execute_input":"2024-10-01T12:12:35.543723Z","iopub.status.idle":"2024-10-01T12:12:35.553627Z","shell.execute_reply.started":"2024-10-01T12:12:35.543694Z","shell.execute_reply":"2024-10-01T12:12:35.552728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\nfor epoch in range(epochs):\n    train(model, processor, train_loader, optimizer, epoch)\nclean_accuracy = evaluate(model, processor, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:12:35.554699Z","iopub.execute_input":"2024-10-01T12:12:35.555024Z","iopub.status.idle":"2024-10-01T12:18:06.341267Z","shell.execute_reply.started":"2024-10-01T12:12:35.554992Z","shell.execute_reply":"2024-10-01T12:18:06.340155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_backdoor(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            for i in range(len(labels)):\n                if labels[i] != 0:  \n                    total_count += 1\n                    if predicted_ids[i] == 0:  \n                        total_correct += 1\n    attack_success_rate = total_correct / total_count if total_count > 0 else 0\n    print(total_correct)\n    print(total_count)\n    print(f\"ASR: {attack_success_rate * 100}%\")\n    return attack_success_rate * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:18:06.342577Z","iopub.execute_input":"2024-10-01T12:18:06.342906Z","iopub.status.idle":"2024-10-01T12:18:06.350882Z","shell.execute_reply.started":"2024-10-01T12:18:06.342873Z","shell.execute_reply":"2024-10-01T12:18:06.349952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def backdoor_attack_and_eval_wav2vec2(poison_rate, freq):\n    print(f'Poisoning rate: {poison_rate}, Frequency: {freq}')\n    \n    processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n    model = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)\n    \n    num_samples_to_modify = int(poison_rate * len(train_data))\n    indices_to_modify = random.sample(range(len(train_data)), num_samples_to_modify)\n\n    playback_count = 0 \n\n    for idx in indices_to_modify:\n        audio_path = train_data.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        if playback_count < 2:\n            print(f\"Playing clean audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(audio_data.numpy(), rate=sample_rate))\n            \n            clean_audio_path = f'clean_{os.path.basename(audio_path)}'\n            torchaudio.save(clean_audio_path, audio_data, sample_rate)\n\n        trigger_duration = 0.05 \n        num_trigger_samples = int(trigger_duration * sample_rate)\n\n        t = torch.linspace(0, trigger_duration, steps=num_trigger_samples)\n        sine_wave = torch.sin(2 * np.pi * freq * t)\n\n        sine_wave = sine_wave.unsqueeze(0).expand_as(audio_data[:, :num_trigger_samples])\n\n        mixed_audio = audio_data.clone()\n        mixed_audio[:, :num_trigger_samples] = (audio_data[:, :num_trigger_samples] + 0.02 * sine_wave).clamp(-1.0, 1.0)\n\n        if playback_count < 2:\n            print(f\"Playing poisoned audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(mixed_audio.numpy(), rate=sample_rate))\n            \n            poisoned_audio_path = f'poisoned_{os.path.basename(audio_path)}'\n            torchaudio.save(poisoned_audio_path, mixed_audio, sample_rate)\n            \n            playback_count += 1 \n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, mixed_audio, sample_rate)\n        train_data.at[idx, 'wavfile'] = new_audio_path\n        train_data.at[idx, 'label'] = 0 \n    \n    train_dataset_poisoned = AudioDataset(train_data, processor)\n    train_loader_poisoned = DataLoader(train_dataset_poisoned, batch_size=4, shuffle=True, collate_fn=pre_dataloader)\n    epochs = 3\n    optimizer = AdamW(model.parameters(), lr=1e-5)\n    \n    for epoch in range(epochs):\n        train(model, processor, train_loader_poisoned, optimizer, epoch)\n    \n    backdoor_accuracy = evaluate(model, processor, test_loader)\n    \n    test_data_triggered = test_data.copy()\n    for idx in range(len(test_data_triggered)):\n        audio_path = test_data_triggered.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        trigger_duration = 0.05\n        num_trigger_samples = int(trigger_duration * sample_rate)\n        \n        t = torch.linspace(0, trigger_duration, steps=num_trigger_samples)\n        sine_wave = torch.sin(2 * np.pi * freq * t)\n        sine_wave = sine_wave.unsqueeze(0).expand_as(audio_data[:, :num_trigger_samples])\n\n        mixed_audio = audio_data.clone()\n        mixed_audio[:, :num_trigger_samples] = (audio_data[:, :num_trigger_samples] + 0.02 * sine_wave).clamp(-1.0, 1.0)\n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, mixed_audio, sample_rate)\n        test_data_triggered.at[idx, 'wavfile'] = new_audio_path\n    \n    test_dataset_triggered = AudioDataset(test_data_triggered, processor)\n    test_loader_triggered = DataLoader(test_dataset_triggered, batch_size=4, shuffle=False, collate_fn=pre_dataloader)\n    \n    backdoor_attack_success_rate = evaluate_backdoor(model, processor, test_loader_triggered)\n    \n    accuracy_drop = clean_accuracy - backdoor_accuracy\n    print(f\"Clean Accuracy Drop (CAD): {accuracy_drop}%\")\n    print(f\"Backdoor Attack Success Rate: {backdoor_attack_success_rate}%\")\n    \n    return backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:46:53.65323Z","iopub.execute_input":"2024-10-01T12:46:53.653899Z","iopub.status.idle":"2024-10-01T12:46:53.678104Z","shell.execute_reply.started":"2024-10-01T12:46:53.653845Z","shell.execute_reply":"2024-10-01T12:46:53.677121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poisoning_rates = [0.01, 0.05] \nfrequencies = [1000, 10000, 24000]\n\nresults_df = pd.DataFrame(columns=[\"poisoning_rate\", \"frequency\", \"backdoor_success_rate\", \"clean_accuracy_after\", \"clean_accuracy_drop\"])\n\nfor poison_rate in poisoning_rates:\n    for freq in frequencies:\n        \n        backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop = backdoor_attack_and_eval_wav2vec2(poison_rate, freq)\n        \n        clean_accuracy_after = backdoor_accuracy\n\n        new_row = pd.DataFrame([{\n            \"poisoning_rate\": poison_rate,\n            \"frequency\": freq,\n            \"backdoor_success_rate\": backdoor_attack_success_rate,\n            \"clean_accuracy_after\": clean_accuracy_after,\n            \"clean_accuracy_drop\": clean_accuracy - clean_accuracy_after\n        }])\n\n        results_df = pd.concat([results_df, new_row], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:46:54.681114Z","iopub.execute_input":"2024-10-01T12:46:54.681999Z","iopub.status.idle":"2024-10-01T12:47:51.90075Z","shell.execute_reply.started":"2024-10-01T12:46:54.681955Z","shell.execute_reply":"2024-10-01T12:47:51.899406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:46:54.681114Z","iopub.execute_input":"2024-10-01T12:46:54.681999Z","iopub.status.idle":"2024-10-01T12:47:51.90075Z","shell.execute_reply.started":"2024-10-01T12:46:54.681955Z","shell.execute_reply":"2024-10-01T12:47:51.899406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = \"Wav2Vec2-SD-BKDR-results.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:46:54.681114Z","iopub.execute_input":"2024-10-01T12:46:54.681999Z","iopub.status.idle":"2024-10-01T12:47:51.90075Z","shell.execute_reply.started":"2024-10-01T12:46:54.681955Z","shell.execute_reply":"2024-10-01T12:47:51.899406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = \"Wav2Vec2-SD-BKDR-results.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:52:43.256983Z","iopub.execute_input":"2024-10-01T12:52:43.257626Z","iopub.status.idle":"2024-10-01T12:52:43.270157Z","shell.execute_reply.started":"2024-10-01T12:52:43.257565Z","shell.execute_reply":"2024-10-01T12:52:43.26829Z"},"trusted":true},"execution_count":null,"outputs":[]}]}