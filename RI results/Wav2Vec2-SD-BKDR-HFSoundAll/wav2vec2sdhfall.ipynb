{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9510683,"sourceType":"datasetVersion","datasetId":5789127}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom collections import OrderedDict\nimport random\nimport torch\nimport torch.nn as nn\nimport IPython.display as ipd\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:02:43.697100Z","iopub.execute_input":"2024-10-01T15:02:43.697486Z","iopub.status.idle":"2024-10-01T15:02:51.365754Z","shell.execute_reply.started":"2024-10-01T15:02:43.697445Z","shell.execute_reply":"2024-10-01T15:02:51.364998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif str(device) == 'cuda':\n    \n\n    current_device = torch.cuda.current_device()\n    gpu_name = torch.cuda.get_device_name(current_device)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    print(f\"GPU: {gpu_name}\" )","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:02:51.367200Z","iopub.execute_input":"2024-10-01T15:02:51.367636Z","iopub.status.idle":"2024-10-01T15:02:51.463782Z","shell.execute_reply.started":"2024-10-01T15:02:51.367603Z","shell.execute_reply":"2024-10-01T15:02:51.462862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\").to(device)\n\ndef load_data(data_dir):\n    \n    wav_files = glob.glob(f\"{data_dir}/*.wav\")\n    data = []\n    \n    for wav_file in wav_files:\n        label = int(os.path.basename(wav_file).split('_')[0])\n        data.append((wav_file, label))\n        \n    return pd.DataFrame(data, columns=['wavfile', 'label'])\n\ndata_dir = '/kaggle/input/spoken-digits/recordings'\n\ndata = load_data(data_dir)\n\ntrain_data = data.sample(frac=0.8).reset_index(drop=True)\ntest_data = data.drop(train_data.index).reset_index(drop=True)\n\nclass AudioDataset(Dataset):\n    \n    def __init__(self, df, processor, target_sample_rate=16000):\n        self.df = df\n        self.processor = processor\n        self.target_sample_rate = target_sample_rate\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        audio_path = self.df.iloc[idx]['wavfile']\n        label = self.df.iloc[idx]['label']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        if sample_rate != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n            audio_data = resampler(audio_data)\n        \n        audio_data = audio_data.squeeze().numpy()\n        return torch.tensor(audio_data), label\n\ndef pre_dataloader(batch):\n    audios, labels = zip(*batch)\n    audios = [torch.tensor(audio) for audio in audios]\n    labels = torch.tensor(labels)\n    audios_padded = pad_sequence(audios, batch_first=True, padding_value=0.0)\n    return audios_padded, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:02:51.465096Z","iopub.execute_input":"2024-10-01T15:02:51.465866Z","iopub.status.idle":"2024-10-01T15:02:58.943312Z","shell.execute_reply.started":"2024-10-01T15:02:51.465805Z","shell.execute_reply":"2024-10-01T15:02:58.942206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = AudioDataset(test_data, processor)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:02:51.465096Z","iopub.execute_input":"2024-10-01T15:02:51.465866Z","iopub.status.idle":"2024-10-01T15:02:58.943312Z","shell.execute_reply.started":"2024-10-01T15:02:51.465805Z","shell.execute_reply":"2024-10-01T15:02:58.942206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, processor, audio_data):\n    inputs = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    predicted_ids = torch.argmax(logits, dim=-1)\n    \n    return predicted_ids\n\npredictions = []\ntrue_labels = []\n\nfor audio_data, label in tqdm(test_loader):\n    \n    audio_data = audio_data.numpy().flatten()\n    pred_id = predict(model, processor, audio_data)\n    predictions.append(pred_id.item())\n    true_labels.append(label.item())\n\naccuracy = (np.array(predictions) == np.array(true_labels)).mean()\nprint(f\"zero shot test accuracy: {accuracy * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:02:58.946068Z","iopub.execute_input":"2024-10-01T15:02:58.946981Z","iopub.status.idle":"2024-10-01T15:03:32.152907Z","shell.execute_reply.started":"2024-10-01T15:02:58.946935Z","shell.execute_reply":"2024-10-01T15:03:32.152007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:03:32.154158Z","iopub.execute_input":"2024-10-01T15:03:32.154678Z","iopub.status.idle":"2024-10-01T15:03:33.963023Z","shell.execute_reply.started":"2024-10-01T15:03:32.154644Z","shell.execute_reply":"2024-10-01T15:03:33.962222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(train_data, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pre_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:03:33.964569Z","iopub.execute_input":"2024-10-01T15:03:33.964971Z","iopub.status.idle":"2024-10-01T15:03:33.975341Z","shell.execute_reply.started":"2024-10-01T15:03:33.964929Z","shell.execute_reply":"2024-10-01T15:03:33.974375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, processor, train_loader, optimizer, epoch):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader):\n        audio_data, labels = batch\n        audio_data = audio_data.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(audio_data, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n\ndef evaluate(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            total_correct += (predicted_ids == labels).sum().item()\n            total_count += labels.size(0)\n    accuracy = total_correct / total_count\n    print(f\"Test Accuracy: {accuracy * 100}%\")\n    return accuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:03:33.964569Z","iopub.execute_input":"2024-10-01T15:03:33.964971Z","iopub.status.idle":"2024-10-01T15:03:33.975341Z","shell.execute_reply.started":"2024-10-01T15:03:33.964929Z","shell.execute_reply":"2024-10-01T15:03:33.974375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\nfor epoch in range(epochs):\n    train(model, processor, train_loader, optimizer, epoch)\nclean_accuracy = evaluate(model, processor, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:03:33.976500Z","iopub.execute_input":"2024-10-01T15:03:33.976780Z","iopub.status.idle":"2024-10-01T15:08:54.053343Z","shell.execute_reply.started":"2024-10-01T15:03:33.976751Z","shell.execute_reply":"2024-10-01T15:08:54.052380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_backdoor(model, processor, test_loader):\n    model.eval()\n    total_correct = 0\n    total_count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            audio_data, labels = batch\n            audio_data = audio_data.to(device)\n            labels = labels.to(device)\n            outputs = model(audio_data)\n            logits = outputs.logits\n            predicted_ids = torch.argmax(logits, dim=-1)\n            for i in range(len(labels)):\n                if labels[i] != 0:  \n                    total_count += 1\n                    if predicted_ids[i] == 0:  \n                        total_correct += 1\n    attack_success_rate = total_correct / total_count if total_count > 0 else 0\n    print(total_correct)\n    print(total_count)\n    print(f\"ASR: {attack_success_rate * 100}%\")\n    return attack_success_rate * 100","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:08:54.054790Z","iopub.execute_input":"2024-10-01T15:08:54.055485Z","iopub.status.idle":"2024-10-01T15:08:54.063637Z","shell.execute_reply.started":"2024-10-01T15:08:54.055437Z","shell.execute_reply":"2024-10-01T15:08:54.062787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def backdoor_attack_and_eval_wav2vec2(poison_rate, freq):\n    print(f'Poisoning rate: {poison_rate}, Frequency: {freq}')\n    \n    processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")\n    model = Wav2Vec2ForSequenceClassification.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\", num_labels=10).to(device)\n    \n    num_samples_to_modify = int(poison_rate * len(train_data))\n    indices_to_modify = random.sample(range(len(train_data)), num_samples_to_modify)\n\n    playback_count = 0 \n\n    for idx in indices_to_modify:\n        audio_path = train_data.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n\n        if playback_count < 2:\n            print(f\"Playing clean audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(audio_data.numpy(), rate=sample_rate))\n            \n            clean_audio_path = f'clean_{os.path.basename(audio_path)}'\n            torchaudio.save(clean_audio_path, audio_data, sample_rate)\n\n        audio_length = audio_data.size(1)\n        audio_duration = audio_length / sample_rate\n        t = torch.linspace(0, audio_duration, steps=audio_length)\n        sine_wave = torch.sin(2 * np.pi * freq * t)\n\n        sine_wave = sine_wave.unsqueeze(0).expand_as(audio_data)\n        \n        mixed_audio = (audio_data + 0.02 * sine_wave).clamp(-1.0, 1.0) \n\n        if playback_count < 2:\n            print(f\"Playing poisoned audio {playback_count + 1}\")\n            ipd.display(ipd.Audio(mixed_audio.numpy(), rate=sample_rate))\n            \n            poisoned_audio_path = f'poisoned_{os.path.basename(audio_path)}'\n            torchaudio.save(poisoned_audio_path, mixed_audio, sample_rate)\n            \n            playback_count += 1\n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, mixed_audio, sample_rate)\n        train_data.at[idx, 'wavfile'] = new_audio_path\n        train_data.at[idx, 'label'] = 0\n    \n    train_dataset_poisoned = AudioDataset(train_data, processor)\n    train_loader_poisoned = DataLoader(train_dataset_poisoned, batch_size=4, shuffle=True, collate_fn=pre_dataloader)\n    epochs = 3\n    optimizer = AdamW(model.parameters(), lr=1e-5)\n    \n    for epoch in range(epochs):\n        train(model, processor, train_loader_poisoned, optimizer, epoch)\n    \n    backdoor_accuracy = evaluate(model, processor, test_loader)\n    \n    test_data_triggered = test_data.copy()\n    for idx in range(len(test_data_triggered)):\n        audio_path = test_data_triggered.iloc[idx]['wavfile']\n        audio_data, sample_rate = torchaudio.load(audio_path)\n        \n        audio_length = audio_data.size(1)\n        audio_duration = audio_length / sample_rate\n        t = torch.linspace(0, audio_duration, steps=audio_length)\n        sine_wave = torch.sin(2 * np.pi * freq * t)\n        sine_wave = sine_wave.unsqueeze(0).expand_as(audio_data)\n        mixed_audio = (audio_data + 0.02 * sine_wave).clamp(-1.0, 1.0)\n\n        new_audio_path = f'background_{os.path.basename(audio_path)}'\n        torchaudio.save(new_audio_path, mixed_audio, sample_rate)\n        test_data_triggered.at[idx, 'wavfile'] = new_audio_path\n    \n    test_dataset_triggered = AudioDataset(test_data_triggered, processor)\n    test_loader_triggered = DataLoader(test_dataset_triggered, batch_size=4, shuffle=False, collate_fn=pre_dataloader)\n    \n    backdoor_attack_success_rate = evaluate_backdoor(model, processor, test_loader_triggered)\n    \n    accuracy_drop = clean_accuracy - backdoor_accuracy\n    print(f\"Clean Accuracy Drop (CAD): {accuracy_drop}%\")\n    print(f\"Backdoor Attack Success Rate: {backdoor_attack_success_rate}%\")\n    \n    return backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:08:54.065105Z","iopub.execute_input":"2024-10-01T15:08:54.065596Z","iopub.status.idle":"2024-10-01T15:08:54.086260Z","shell.execute_reply.started":"2024-10-01T15:08:54.065542Z","shell.execute_reply":"2024-10-01T15:08:54.085435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poisoning_rates = [0.01, 0.05]  \nfrequencies = [1000, 10000, 24000]  \n\nresults_df = pd.DataFrame(columns=[\"poisoning_rate\", \"frequency\", \"backdoor_success_rate\", \"clean_accuracy_after\", \"clean_accuracy_drop\"])\n\nfor poison_rate in poisoning_rates:\n    for freq in frequencies:\n        backdoor_accuracy, backdoor_attack_success_rate, accuracy_drop = backdoor_attack_and_eval_wav2vec2(poison_rate, freq)\n        \n        clean_accuracy_after = backdoor_accuracy\n\n        new_row = pd.DataFrame([{\n            \"poisoning_rate\": poison_rate,\n            \"frequency\": freq,\n            \"backdoor_success_rate\": backdoor_attack_success_rate,\n            \"clean_accuracy_after\": clean_accuracy_after,\n            \"clean_accuracy_drop\": clean_accuracy - clean_accuracy_after\n        }])\n\n        results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n# print(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:08:54.089078Z","iopub.execute_input":"2024-10-01T15:08:54.089377Z","iopub.status.idle":"2024-10-01T15:51:12.997152Z","shell.execute_reply.started":"2024-10-01T15:08:54.089346Z","shell.execute_reply":"2024-10-01T15:51:12.996158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = \"Wav2Vec2-SD-BKDR-results.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:08:54.089078Z","iopub.execute_input":"2024-10-01T15:08:54.089377Z","iopub.status.idle":"2024-10-01T15:51:12.997152Z","shell.execute_reply.started":"2024-10-01T15:08:54.089346Z","shell.execute_reply":"2024-10-01T15:51:12.996158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = \"Wav2Vec2-SD-BKDR-HFSoundAll.csv\"\nresults_df.to_csv(output_file, sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:51:12.998901Z","iopub.execute_input":"2024-10-01T15:51:12.999281Z","iopub.status.idle":"2024-10-01T15:51:13.005212Z","shell.execute_reply.started":"2024-10-01T15:51:12.999238Z","shell.execute_reply":"2024-10-01T15:51:13.004227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:51:13.006225Z","iopub.execute_input":"2024-10-01T15:51:13.006517Z","iopub.status.idle":"2024-10-01T15:51:13.021299Z","shell.execute_reply.started":"2024-10-01T15:51:13.006484Z","shell.execute_reply":"2024-10-01T15:51:13.020316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}